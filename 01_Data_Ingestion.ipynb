{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3dbe334-434a-4277-8160-3241055bd0db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing: products...\nSuccess! Table 'products' is ready.\nNow processing: aisles...\nSuccess! Table 'aisles' is ready.\nNow processing: departments...\nSuccess! Table 'departments' is ready.\nNow processing: order_products__train...\nSuccess! Table 'order_products__train' is ready.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the paths for your data\n",
    "volume_path = \"/Volumes/catalog1/instacart_db/raw_data/\"\n",
    "schema_name = \"catalog1.instacart_db\"\n",
    "\n",
    "# 2. List of the 4 files you currently have uploaded\n",
    "files_to_process = [\"products\", \"aisles\", \"departments\", \"order_products__train\"]\n",
    "\n",
    "# 3. Loop through each file to read and save it\n",
    "for file in files_to_process:\n",
    "    print(f\"Now processing: {file}...\")\n",
    "    \n",
    "    # Read the CSV\n",
    "    # header=true: uses the first row for column names\n",
    "    # inferSchema=true: automatically detects data types (integers, strings, etc.)\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(f\"{volume_path}{file}.csv\")\n",
    "    \n",
    "    # Save as a Delta Table\n",
    "    # mode(\"overwrite\"): replaces the table if it already exists\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema_name}.{file}\")\n",
    "    \n",
    "    print(f\"Success! Table '{file}' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02a1754-4d36-442b-a659-fd3747a0395f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>product_id</th><th>add_to_cart_order</th><th>reordered</th></tr></thead><tbody><tr><td>1</td><td>49302</td><td>1</td><td>1</td></tr><tr><td>1</td><td>11109</td><td>2</td><td>1</td></tr><tr><td>1</td><td>10246</td><td>3</td><td>0</td></tr><tr><td>1</td><td>49683</td><td>4</td><td>0</td></tr><tr><td>1</td><td>43633</td><td>5</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         49302,
         1,
         1
        ],
        [
         1,
         11109,
         2,
         1
        ],
        [
         1,
         10246,
         3,
         0
        ],
        [
         1,
         49683,
         4,
         0
        ],
        [
         1,
         43633,
         5,
         1
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "order_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "product_id",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "add_to_cart_order",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "reordered",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "add_to_cart_order",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "reordered",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Check the products table\n",
    "SELECT * FROM catalog1.instacart_db.order_products__train LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ca0ab2-241f-45d4-96ed-b3c39de85881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process: orders. Please wait, these are large files...\nFinished! Table 'orders' is now ready in your catalog.\nStarting to process: order_products__prior. Please wait, these are large files...\nFinished! Table 'order_products__prior' is now ready in your catalog.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup paths\n",
    "volume_path = \"/Volumes/catalog1/instacart_db/raw_data/\"\n",
    "schema_name = \"catalog1.instacart_db\"\n",
    "\n",
    "# 2. Identify the final 2 files\n",
    "final_files = [\"orders\", \"order_products__prior\"]\n",
    "\n",
    "# 3. Process each file\n",
    "for file in final_files:\n",
    "    print(f\"Starting to process: {file}. Please wait, these are large files...\")\n",
    "    \n",
    "    # Read the CSV from the Volume\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(f\"{volume_path}{file}.csv\")\n",
    "    \n",
    "    # Save as a permanent Delta Table\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema_name}.{file}\")\n",
    "    \n",
    "    print(f\"Finished! Table '{file}' is now ready in your catalog.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b0872e-eb8a-48c7-8012-a273dd5fa709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Name                | Row Count      \n---------------------------------------------\nproducts                  | 49,688         \naisles                    | 134            \ndepartments               | 21             \norder_products__train     | 1,384,617      \norders                    | 3,421,083      \norder_products__prior     | 32,434,489     \n"
     ]
    }
   ],
   "source": [
    "# List of all your tables\n",
    "all_tables = [\"products\", \"aisles\", \"departments\", \"order_products__train\", \"orders\", \"order_products__prior\"]\n",
    "\n",
    "print(f\"{'Table Name':<25} | {'Row Count':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for table in all_tables:\n",
    "    # Use spark.table to grab the table and .count() to get the total rows\n",
    "    count = spark.table(f\"catalog1.instacart_db.{table}\").count()\n",
    "    print(f\"{table:<25} | {count:<15,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6861377-1284-4bd0-a0cd-7a75573eea6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts in Orders table:\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n|       0|      0|       0|           0|        0|                0|                206209|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# We will check the 'orders' table specifically as it's the most important\n",
    "orders_df = spark.table(\"catalog1.instacart_db.orders\")\n",
    "\n",
    "# This code counts how many NULL values are in each column\n",
    "null_counts = orders_df.select([count(when(col(c).isNull(), c)).alias(c) for c in orders_df.columns])\n",
    "\n",
    "print(\"Null value counts in Orders table:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c032c934-bdeb-40de-8efd-a39472384016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! The 'orders' table is now clean and null-free.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "# 1. Update the dataframe to replace NULLs with 0\n",
    "# coalesce returns the first non-null value it finds\n",
    "orders_clean_df = orders_df.withColumn(\n",
    "    \"days_since_prior_order\", \n",
    "    coalesce(col(\"days_since_prior_order\"), lit(0))\n",
    ")\n",
    "\n",
    "# 2. Overwrite the permanent table with our clean version\n",
    "orders_clean_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"catalog1.instacart_db.orders\")\n",
    "\n",
    "print(\"Success! The 'orders' table is now clean and null-free.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e64fa4-6d80-43e7-90bf-088283d31048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts in Orders table:\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n|       0|      0|       0|           0|        0|                0|                     0|\n+--------+-------+--------+------------+---------+-----------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# We will check the 'orders' table specifically as it's the most important\n",
    "orders_df = spark.table(\"catalog1.instacart_db.orders\")\n",
    "\n",
    "# This code counts how many NULL values are in each column\n",
    "null_counts = orders_df.select([count(when(col(c).isNull(), c)).alias(c) for c in orders_df.columns])\n",
    "\n",
    "print(\"Null value counts in Orders table:\")\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe13ed1-3f6d-402e-a5af-2a26bd8d29ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6786930265257882,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Data_Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}